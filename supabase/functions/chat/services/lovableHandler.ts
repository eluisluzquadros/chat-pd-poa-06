import { ChatResponse } from "../types.ts";

// Fun√ß√£o para estimar tokens (aproxima√ß√£o: ~4 chars = 1 token)
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

// Fun√ß√£o para truncar hist√≥rico mantendo limite de tokens
function truncateHistory(history: any[], maxTokens: number): any[] {
  const truncated: any[] = [];
  let currentTokens = 0;
  
  // Adicionar mensagens do mais recente para o mais antigo
  for (let i = history.length - 1; i >= 0; i--) {
    const msgTokens = estimateTokens(history[i].content || '');
    if (currentTokens + msgTokens > maxTokens) {
      break;
    }
    truncated.unshift(history[i]);
    currentTokens += msgTokens;
  }
  
  return truncated;
}

export async function processLovableAgent(
  message: string,
  sessionId: string | null,
  agent: any,
  apiKey: string
): Promise<ChatResponse> {
  
  console.log(`üìù Processing with Lovable agent: ${agent.display_name}`);
  
  const systemPrompt = agent.parameters?.system_prompt || "You are a helpful assistant.";
  const maxCompletionTokens = agent.parameters?.max_tokens || 4000;
  const temperature = agent.parameters?.temperature || 0.7;
  const maxHistoryMessages = agent.parameters?.max_history_messages || 30;
  
  // Buscar hist√≥rico da sess√£o (se houver)
  let conversationHistory: any[] = [];
  
  if (sessionId) {
    const { createClient } = await import("https://esm.sh/@supabase/supabase-js@2");
    const supabase = createClient(
      Deno.env.get("SUPABASE_URL") ?? "",
      Deno.env.get("SUPABASE_SERVICE_ROLE_KEY") ?? ""
    );
    
    const { data: history } = await supabase
      .from('chat_history')
      .select('message')
      .eq('session_id', sessionId)
      .order('created_at', { ascending: true })
      .limit(maxHistoryMessages);
    
    if (history) {
      conversationHistory = history.map(h => h.message);
      
      // Controle de tokens: reservar 2000 tokens para hist√≥rico
      const maxHistoryTokens = 2000;
      const historyTokenCount = conversationHistory.reduce(
        (total, msg) => total + estimateTokens(msg.content || ''), 
        0
      );
      
      if (historyTokenCount > maxHistoryTokens) {
        console.log(`‚ö†Ô∏è History tokens (${historyTokenCount}) exceeded limit, truncating...`);
        conversationHistory = truncateHistory(conversationHistory, maxHistoryTokens);
        console.log(`‚úÇÔ∏è Truncated to ${conversationHistory.length} messages`);
      }
    }
  }
  
  // Montar mensagens para OpenAI Completions API
  const messages = [
    { role: "system", content: systemPrompt },
    ...conversationHistory,
    { role: "user", content: message }
  ];
  
  console.log(`üîÑ Calling OpenAI Completions API with ${messages.length} messages`);
  
  // Chamar OpenAI Completions API
  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${apiKey}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      model: agent.model || "gpt-4o-mini",
      messages: messages,
      max_completion_tokens: maxCompletionTokens,
      temperature: temperature,
      stream: false
    })
  });
  
  if (!response.ok) {
    const errorText = await response.text();
    console.error(`‚ùå OpenAI API Error: ${response.status} - ${errorText}`);
    throw new Error(`OpenAI API Error: ${response.status} - ${errorText}`);
  }
  
  const data = await response.json();
  const assistantMessage = data.choices[0]?.message?.content || "No response generated";
  
  console.log(`‚úÖ Lovable agent response generated (${assistantMessage.length} chars)`);
  
  return {
    content: assistantMessage,
    threadId: sessionId || "no-thread",
    progressMessages: []
  };
}
